# <p align="center">_**SimMark**_: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models</p>

<p align="center">
  <br>
  <a href="??"><img alt="Paper" src="https://img.shields.io/badge/ðŸ“ƒ-Paper-808080"></a>
  <a href="https://simmark-llm.github.io/"><img alt="Website" src="https://img.shields.io/badge/%F0%9F%8C%90-Website-008080"></a>
</p>

## Abstract
The rapid proliferation of large language models (LLMs) has created an urgent need for reliable methods to detect whether a text is generated by such models. 
In this paper, we propose _**SimMark**_, a posthoc watermarking algorithm that makes LLMs' outputs traceable without requiring access to the model's internal logits, enabling compatibility with a wide range of LLMs, including API-only models.
By leveraging the similarity of semantic sentence embeddings and rejection sampling to impose detectable statistical patterns imperceptible to humans, and employing a _soft_ counting mechanism, _SimMark_ achieves robustness against paraphrasing attacks.
Experimental results demonstrate that _SimMark_ sets a new benchmark for robust watermarking of LLM-generated content, surpassing prior sentence-level watermarking techniques in robustness, sampling efficiency, and applicability across diverse domains, all while preserving the text quality.
<div align="center">
  <img src="https://github.com/user-attachments/assets/1550a812-fd1b-49a4-bb38-8a63b207773b" alt="image" width="70%">
</div>

The input text is divided into individual sentences $`X_1`$ to $`X_N`$, which are embedded using a semantic embedding model, with optional PCA for dimensionality reduction. 
The similarity between consecutive sentence embeddings is computed. Sentences with similarities within a predefined interval $`[a, b]`$ are considered $`\color{ForestGreen}\textbf{valid}`$, while those outside are $`\textcolor{BrickRed}{\textbf{invalid}}`$. 
A _**soft**_-$`z`$-test is performed using the _soft_ count of $`\textcolor{BrickRed}{\mathrm{invalid/partially-valid}}`$ sentences to determine whether the text is watermarked.

## Overview of SimMark: A Similarity-Based Watermark
<div align="center">
  <img src="https://github.com/user-attachments/assets/ed15124e-8c33-44fe-8363-dcad810d9f59" alt="image">
</div>
Top: ***Generation***.
For each newly generated sentence ($X_{i+1}$), its embedding ($e_{i+1}$) is computed using the Instructor-Large model, optionally applying PCA for dimensionality reduction.  
The cosine similarity (or Euclidean distance) between $e_{i+1}$ and the embedding of the previous sentence ($e_i$), denoted as $s_{i+1}$, is calculated. If $s_{i+1}$ lies within the predefined interval $`[a, b]`$, the sentence is marked $`\color{ForestGreen}\textbf{valid}`$ and accepted.  
Otherwise, rejection sampling generates a new candidate sentence until validity is achieved or the iteration limit is reached. Once a sentence is accepted, the process repeats for subsequent sentences.  

Bottom: ***Detection (+ Paraphrase attack)***.  Paraphrased versions of watermarked sentences are generated ($Y_{i}$), and their embeddings ($`e'_{i}`$) are computed. The similarity (or distance) between consecutive sentences in the paraphrased text is evaluated. If paraphrasing causes the similarity ($`s'_{i+1}`$) to fall outside $`[a, b]`$, it is mismarked as $`\textcolor{BrickRed}{\textbf{invalid}}`$.  
A *soft counting* mechanism (via function $c(s_{i+1})$ instead of a regular counting with a step function in the interval $`[a, b]`$) quantifies partial validity based on proximity to the interval bounds, enabling detection of watermarked text via the ***soft***-$`z`$-test even under paraphrase attacks.  
It should be emphasized that soft counting is always applied during detection, regardless of whether paraphrasing is present or not, as we cannot assume prior knowledge of paraphrasing.

## Performance
<div align="center">
  <img src="https://github.com/user-attachments/assets/7a947de7-f1b9-4a86-a1a3-4f2391c57420" alt="image">
</div>
Performance of different algorithms across datasets and paraphrasers, evaluated using <b>ROC-AUCâ†‘</b>, <b>TP@FP=1%â†‘</b>, and <b>TP@FP=5%â†‘</b>, respectively, reported from left to right. Higher values indicate better performance across all metrics.
In each column, <b>bold</b> values indicate the best performance for a given dataset and metric, while <u>underlined</u> values denote the second-best. 
**<i>SimMark</i> consistently outperforms or is on par with other state-of-the-art methods across datasets and paraphrasers, and it is the best on average.**

## How To Run?
### Installation
1. Clone this repository:
```
git clone https://github.com/DabiriAghdam/SimMark.git
```
2. Create a virtual environment (recommended), and then install dependencies: 
```
    pip3 install -r requirements.txt
```
3. Then run following commands to download the necessary data:
```
    python3 load_punkt_tab.py
    python3 load_c4.py
    python3 load_booksum.py
    python3 load_tifu.py
```
### Reproducing our results (requires GPU)
#### For RealNews dataset as an example you can run the follwing:
1. **Without paraphrase**:
  - _Cosine-SimMark_:  
  ```
  python3 detection.py watermarked/c4/c4-cosine  --human_text human/c4  --mode cosine --a 0.68 --b 0.76 
  ```
  - _Euclidean-SimMark_:  
  ```
  python3 detection.py watermarked/c4/c4-euclidean-pca  --human_text human/c4  --mode euclidean --a 0.28 --b 0.36 --use_pca
  ```
2. **With Pegasus paraphraser**:
  - _Cosine-SimMark_:   
  ```
  python3 detection.py watermarked/c4/c4-cosine-pegasus --human_text human/c4  --mode cosine --a 0.68 --b 0.76 
  ```
  - _Euclidean-SimMark_:  
  ```
  python3 detection.py watermarked/c4/c4-euclidean-pca-pegasus  --human_text human/c4  --mode euclidean --a 0.28 --b 0.36 --use_pca    
  ```  
3. Additional paraphrasers data can be used similarly by changing the dataset path.

  #### For other datasets, just replace the dataset paths and make sure you set the correct parameters for "human_text", "mode", "a", "b", and if necessary add "--use_pca" flag.
4. For example for the BookSum dataset with Pegasus paraphraser:
  - _Cosine-SimMark_:   
  ```
  python3 detection.py watermarked/booksum/booksum-cosine-pegasus --human_text human/booksum  --mode cosine --a 0.68 --b 0.76
  ```
  - _Euclidean-SimMark_:  
  ```
  python3 detection.py watermarked/booksum/booksum-euclidean-pegasus --human_text human/booksum  --mode euclidean --a 0.4 --b 0.55
  ```
### Hyperparameters
The key parameters that can be used with detection.py are summarized as follows (for more details, see the code itself):
- Data path: See the watermarked folder.
- Mode (--mode): 'cosine' or 'euclidean'
- Predefined Intervals ([a, b]):
  - RealNews dataset (human_text should be human/c4):
    - Cosine Similarity: [0.68, 0.76]
    - Euclidean Distance: [0.28, 0.36] (must add --use_pca flag)
  - BookSum dataset (human_text should be human/booksum):
    - Cosine Similarity: [0.68, 0.76]
    - Euclidean Distance: [0.4, 0.55] (DO NOT add --use_pca flag)
  - Reddit-TIFU dataset (human_text should be human/tifu):
      - Cosine Similarity: [0.68, 0.76]
      - Euclidean Distance: [0.28, 0.36] (must add --use_pca flag)
- Soft Count Decay Factor (--K): 250 is the default.
- LLM (--model_path): 'AbeHou/opt-1.3b-semstamp' is the default.
- Embedding Model (--embedder): 'hkunlp/instructor-large' is the default.

For further details on hyperparameter selection, refer to the paper.

### Text Quality Evaluation
For text quality evaluation, you can run the following command for Euclidean-SimMark on the BookSum dataset for instance:
```
python3 eval_quality.py --dataset_name watermarked/booksum/booksum-euclidean --human_ref_name human/booksum
```
### Generating New Watermarked Data (requires GPU)
  Coming Soon!

#### Acknowledgement: 
Some of the codes were partially adapted from these work (original github link: https://github.com/bohanhou14/semstamp):

  - "SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation"  (https://arxiv.org/abs/2310.03991)

  - "k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text"  (https://arxiv.org/abs/2402.11399)
  
## Citation
If you found this repository helpful, please don't forget to cite our paper:
```BibTeX
@misc{dabiriaghdam2025simmarkrobustsentencelevelsimilaritybased,
      title={SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models}, 
      author={Amirhossein Dabiriaghdam and Lele Wang},
      year={2025},
      eprint={2502.02787},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.02787}, 
}
```
If you have any questions, please feel free to contact  [amirhossein@ece.ubc.ca](mailto:amirhossein@ece.ubc.ca).
